import subprocess

def read_urls_from_file(filename):
    urls = []
    with open(filename, 'r') as file:
        for line in file:
            url = line.strip()
            if url:
                urls.append(url)
    return urls

def access_websites_in_loop_with_curl(urls):
    index = 0
    while True:
        url = urls[index]
        try:
            # Using subprocess to execute curl command with follow redirects (-L) and user-agent header (-A)
            result = subprocess.run(["curl", "-s", "-o", "/dev/null", "-w", "%{http_code}", "-L", "-A", "Mozilla/5.0", url], capture_output=True, text=True)
            status_code = int(result.stdout.strip())
            if status_code == 200:
                print("Accessed {} successfully".format(url))
            elif status_code == 301:
                print("Failed to access {}. Status code: {} (Moved Permanently)".format(url, status_code))
            elif status_code == 403:
                print("Failed to access {}. Status code: {} (Forbidden)".format(url, status_code))
            else:
                print("Failed to access {}. Status code: {}".format(url, status_code))
        except subprocess.CalledProcessError as e:
            print("Failed to access {}. Error: {}".format(url, e))

        index = (index + 1) % len(urls)  # Move to the next URL, looping back to the start if at the end

if __name__ == "__main__":
    filename = "urls.txt"  # Replace with your file name containing URLs
    urls = read_urls_from_file(filename)
    access_websites_in_loop_with_curl(urls)
